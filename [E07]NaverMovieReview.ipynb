{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "another-distance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "\n",
    "# 데이터를 읽어봅시다. \n",
    "train_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_train.txt')\n",
    "test_data = pd.read_table('~/aiffel/sentiment_classification/data/ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "expired-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "tokenizer = Mecab()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "\n",
    "def load_data(train_data, test_data, num_words=10000):\n",
    "    train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    train_data = train_data.dropna(how = 'any') \n",
    "    test_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    test_data = test_data.dropna(how = 'any') \n",
    "    \n",
    "    X_train = []\n",
    "    for sentence in train_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_train.append(temp_X)\n",
    "\n",
    "    X_test = []\n",
    "    for sentence in test_data['document']:\n",
    "        temp_X = tokenizer.morphs(sentence) # 토큰화\n",
    "        temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "        X_test.append(temp_X)\n",
    "    \n",
    "    words = np.concatenate(X_train).tolist() # ?\n",
    "    counter = Counter(words)\n",
    "    counter = counter.most_common(10000 - 4)\n",
    "    vocab = ['<PAD>', '<BOS>', '<UNK>', '<UNUSED>'] + [key for key, _ in counter]\n",
    "    word_to_index = {word:index for index, word in enumerate(vocab)}\n",
    "    word_to_index_cp = word_to_index\n",
    "        \n",
    "    def wordlist_to_indexlist(wordlist):\n",
    "        return [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in wordlist]\n",
    "        \n",
    "    X_train = list(map(wordlist_to_indexlist, X_train))\n",
    "    X_test = list(map(wordlist_to_indexlist, X_test))\n",
    "        \n",
    "    return X_train, np.array(list(train_data['label'])), X_test, np.array(list(test_data['label'])), word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "brief-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "mature-dance",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "graphic-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트 벡터로 변환해 주는 함수입니다. \n",
    "# 단, 모든 문장은 <BOS>로 시작하는 것으로 합니다. \n",
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 단어 인덱스 리스트 벡터로 encode해 주는 함수입니다. \n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode된 문장을 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])  #[1:]를 통해 <BOS>를 제외\n",
    "\n",
    "# 여러 개의 숫자 벡터로 encode된 문장을 한꺼번에 원래대로 decode하는 함수입니다. \n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "human-consolidation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  15.96940191154864\n",
      "문장길이 최대 :  116\n",
      "문장길이 표준편차 :  12.843571191092\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 테스트 데이터\n",
    "total_data_text = list(X_train) + list(X_test)\n",
    "\n",
    "# 텍스트데이터 문장길이의 리스트를 생성\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산 \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 최대 길이(maxlen)를 (평균 + 2 * 표준편차)로 설정 \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-factor",
   "metadata": {},
   "source": [
    "train과 test에 패딩을 추가하면서 나눴다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "eastern-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train,\n",
    "                                                        value=word_to_index['<PAD>'],\n",
    "                                                        padding='pre', # 혹은 'pre'\n",
    "                                                        maxlen=maxlen)\n",
    "\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test,\n",
    "                                                       value=word_to_index['<PAD>'],\n",
    "                                                       padding='pre', # 혹은 'pre'\n",
    "                                                       maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "muslim-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 약 8:2 비율로 \n",
    "# validation set 30000건 분리\n",
    "X_val = X_train[:30000]   \n",
    "y_val = y_train[:30000]\n",
    "\n",
    "# validation set을 제외한 나머지 \n",
    "train_x = X_train[30000:]  \n",
    "train_y = y_train[30000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-zambia",
   "metadata": {},
   "outputs": [],
   "source": [
    "학습방법 3가지의 정확성을 확인해보았다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-republic",
   "metadata": {},
   "source": [
    "1-D Convolution Neural Network(1-D CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "conventional-trouble",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_33 (Embedding)     (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, None, 16)          1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_19 (Glo (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 163,761\n",
      "Trainable params: 163,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000 # 어휘 사전의 크기입니다(10개의 단어)\n",
    "word_vector_dim = 16   # 단어 하나를 표현하는 임베딩 벡터의 차원 수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "comparative-worcester",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "227/227 [==============================] - 3s 8ms/step - loss: 0.6094 - accuracy: 0.6526 - val_loss: 0.3559 - val_accuracy: 0.8443\n",
      "Epoch 2/10\n",
      "227/227 [==============================] - 2s 9ms/step - loss: 0.3309 - accuracy: 0.8593 - val_loss: 0.3406 - val_accuracy: 0.8529\n",
      "Epoch 3/10\n",
      "227/227 [==============================] - 2s 8ms/step - loss: 0.2862 - accuracy: 0.8819 - val_loss: 0.3403 - val_accuracy: 0.8540\n",
      "Epoch 4/10\n",
      "227/227 [==============================] - 2s 8ms/step - loss: 0.2577 - accuracy: 0.8954 - val_loss: 0.3492 - val_accuracy: 0.8523\n",
      "Epoch 5/10\n",
      "227/227 [==============================] - 2s 8ms/step - loss: 0.2217 - accuracy: 0.9157 - val_loss: 0.3727 - val_accuracy: 0.8470\n",
      "Epoch 6/10\n",
      "227/227 [==============================] - 2s 8ms/step - loss: 0.1867 - accuracy: 0.9315 - val_loss: 0.3982 - val_accuracy: 0.8433\n",
      "Epoch 7/10\n",
      "227/227 [==============================] - 2s 8ms/step - loss: 0.1553 - accuracy: 0.9466 - val_loss: 0.4325 - val_accuracy: 0.8390\n",
      "Epoch 8/10\n",
      "227/227 [==============================] - 2s 8ms/step - loss: 0.1257 - accuracy: 0.9587 - val_loss: 0.4739 - val_accuracy: 0.8350\n",
      "Epoch 9/10\n",
      "227/227 [==============================] - 2s 8ms/step - loss: 0.1025 - accuracy: 0.9678 - val_loss: 0.5189 - val_accuracy: 0.8303\n",
      "Epoch 10/10\n",
      "227/227 [==============================] - 2s 8ms/step - loss: 0.0857 - accuracy: 0.9741 - val_loss: 0.5571 - val_accuracy: 0.8299\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs = 10  \n",
    "\n",
    "history = model.fit(train_x,\n",
    "                    train_y,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "authorized-depression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 2s - loss: 0.5725 - accuracy: 0.8264\n",
      "[0.5724867582321167, 0.8263930082321167]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-launch",
   "metadata": {},
   "source": [
    "GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "federal-rebel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_34 (Embedding)     (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_20 (Glo (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 160,145\n",
      "Trainable params: 160,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원 수 (변경 가능한 하이퍼파라미터)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "partial-cover",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "227/227 [==============================] - 2s 5ms/step - loss: 0.6558 - accuracy: 0.6295 - val_loss: 0.4431 - val_accuracy: 0.8185\n",
      "Epoch 2/10\n",
      "227/227 [==============================] - 1s 5ms/step - loss: 0.4044 - accuracy: 0.8343 - val_loss: 0.3651 - val_accuracy: 0.8400\n",
      "Epoch 3/10\n",
      "227/227 [==============================] - 1s 4ms/step - loss: 0.3337 - accuracy: 0.8605 - val_loss: 0.3522 - val_accuracy: 0.8455\n",
      "Epoch 4/10\n",
      "227/227 [==============================] - 1s 5ms/step - loss: 0.3000 - accuracy: 0.8764 - val_loss: 0.3514 - val_accuracy: 0.8485\n",
      "Epoch 5/10\n",
      "227/227 [==============================] - 1s 4ms/step - loss: 0.2765 - accuracy: 0.8882 - val_loss: 0.3557 - val_accuracy: 0.8478\n",
      "Epoch 6/10\n",
      "227/227 [==============================] - 1s 4ms/step - loss: 0.2560 - accuracy: 0.8976 - val_loss: 0.3622 - val_accuracy: 0.8472\n",
      "Epoch 7/10\n",
      "227/227 [==============================] - 1s 5ms/step - loss: 0.2435 - accuracy: 0.9026 - val_loss: 0.3716 - val_accuracy: 0.8464\n",
      "Epoch 8/10\n",
      "227/227 [==============================] - 1s 4ms/step - loss: 0.2295 - accuracy: 0.9100 - val_loss: 0.3813 - val_accuracy: 0.8452\n",
      "Epoch 9/10\n",
      "227/227 [==============================] - 1s 4ms/step - loss: 0.2169 - accuracy: 0.9163 - val_loss: 0.3920 - val_accuracy: 0.8433\n",
      "Epoch 10/10\n",
      "227/227 [==============================] - 1s 4ms/step - loss: 0.2085 - accuracy: 0.9208 - val_loss: 0.4042 - val_accuracy: 0.8406\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs = 10  \n",
    "\n",
    "history = model.fit(train_x,\n",
    "                    train_y,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "southern-socket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 2s - loss: 0.4167 - accuracy: 0.8353\n",
      "[0.416698157787323, 0.8353235721588135]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-battle",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ecological-carolina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_35 (Embedding)     (None, None, 20)          200000    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 8)                 928       \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 201,009\n",
      "Trainable params: 201,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000  # 어휘 사전의 크기입니다(10000개의 단어)\n",
    "word_vector_dim = 20  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "mediterranean-feeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "227/227 [==============================] - 4s 12ms/step - loss: 0.6137 - accuracy: 0.6786 - val_loss: 0.3763 - val_accuracy: 0.8413\n",
      "Epoch 2/10\n",
      "227/227 [==============================] - 2s 11ms/step - loss: 0.3538 - accuracy: 0.8527 - val_loss: 0.3488 - val_accuracy: 0.8485\n",
      "Epoch 3/10\n",
      "227/227 [==============================] - 2s 11ms/step - loss: 0.3181 - accuracy: 0.8682 - val_loss: 0.3463 - val_accuracy: 0.8493\n",
      "Epoch 4/10\n",
      "227/227 [==============================] - 2s 11ms/step - loss: 0.2989 - accuracy: 0.8760 - val_loss: 0.3451 - val_accuracy: 0.8511\n",
      "Epoch 5/10\n",
      "227/227 [==============================] - 2s 11ms/step - loss: 0.2856 - accuracy: 0.8814 - val_loss: 0.3460 - val_accuracy: 0.8501\n",
      "Epoch 6/10\n",
      "227/227 [==============================] - 2s 10ms/step - loss: 0.2759 - accuracy: 0.8857 - val_loss: 0.3482 - val_accuracy: 0.8497\n",
      "Epoch 7/10\n",
      "227/227 [==============================] - 2s 11ms/step - loss: 0.2616 - accuracy: 0.8918 - val_loss: 0.3554 - val_accuracy: 0.8487\n",
      "Epoch 8/10\n",
      "227/227 [==============================] - 2s 11ms/step - loss: 0.2465 - accuracy: 0.8974 - val_loss: 0.3634 - val_accuracy: 0.8484\n",
      "Epoch 9/10\n",
      "227/227 [==============================] - 2s 11ms/step - loss: 0.2369 - accuracy: 0.9005 - val_loss: 0.3968 - val_accuracy: 0.8479\n",
      "Epoch 10/10\n",
      "227/227 [==============================] - 2s 11ms/step - loss: 0.2221 - accuracy: 0.9075 - val_loss: 0.4048 - val_accuracy: 0.8473\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs = 10  \n",
    "\n",
    "history = model.fit(train_x,\n",
    "                    train_y,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "recent-alfred",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 5s - loss: 0.4138 - accuracy: 0.8425\n",
      "[0.41378098726272583, 0.8424842953681946]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec을 사용해서 다시 추가하고 모델 3개를 다시 돌려보았다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "bibliographic-focus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec 사용 \n",
    "\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "from tensorflow.keras.initializers import Constant\n",
    "import gensim\n",
    "\n",
    "word2vec_path =  os.getenv('HOME')+\"/aiffel/sentiment_classification/ko.bin\"\n",
    "f = open(word2vec_path, 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-4, word_vector_dim)) \n",
    "# 단어 개수(에서 특수문자 4개는 제외하고)만큼의 워드 벡터를 파일에 기록\n",
    "vectors = model.get_weights()[0]\n",
    "for i in range(4,vocab_size):\n",
    "    f.write('{} {}\\n'.format(index_to_word[i], ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()\n",
    "\n",
    "word_vectors = Word2VecKeyedVectors.load_word2vec_format(word2vec_file_path, binary=False)\n",
    "\n",
    "embedding_matrix = np.random.rand(vocab_size, word_vector_dim)\n",
    "\n",
    "# embedding_matrix에 Word2Vec 워드 벡터를 단어 하나씩마다 차례차례 카피\n",
    "for i in range(4,vocab_size):\n",
    "    if index_to_word[i] in word_vectors:\n",
    "        embedding_matrix[i] = word_vectors[index_to_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "sustained-theorem",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('너무', 0.9164605140686035),\n",
       " ('넘', 0.8898469805717468),\n",
       " ('이제훈', 0.8803137540817261),\n",
       " ('야지', 0.8700798153877258),\n",
       " ('씌', 0.8554890155792236),\n",
       " ('드러나', 0.8503690958023071),\n",
       " ('자랑', 0.841502845287323),\n",
       " ('임청하', 0.8413569331169128),\n",
       " ('철저', 0.8336808681488037),\n",
       " ('리그', 0.8323181867599487)]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similar_by_word(\"재미\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-brazil",
   "metadata": {},
   "source": [
    "1-D Convolution Neural Network(1-D CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "suburban-business",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_36 (Embedding)     (None, 41, 20)            200000    \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 35, 16)            2256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 7, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 1, 16)             1808      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_21 (Glo (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 204,209\n",
      "Trainable params: 204,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, \n",
    "                                 word_vector_dim, \n",
    "                                 embeddings_initializer=Constant(embedding_matrix),  # 카피한 임베딩을 여기서 활용\n",
    "                                 input_length=maxlen, \n",
    "                                 trainable=True))   # trainable을 True로 주면 Fine-tuning\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling1D(5))\n",
    "model.add(keras.layers.Conv1D(16, 7, activation='relu'))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid')) \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "mysterious-bullet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "227/227 [==============================] - 3s 9ms/step - loss: 0.5364 - accuracy: 0.7257 - val_loss: 0.3564 - val_accuracy: 0.8463\n",
      "Epoch 2/10\n",
      "227/227 [==============================] - 2s 8ms/step - loss: 0.2935 - accuracy: 0.8806 - val_loss: 0.3493 - val_accuracy: 0.8501\n",
      "Epoch 3/10\n",
      "227/227 [==============================] - 2s 8ms/step - loss: 0.2741 - accuracy: 0.8899 - val_loss: 0.3535 - val_accuracy: 0.8515\n",
      "Epoch 4/10\n",
      "227/227 [==============================] - 2s 8ms/step - loss: 0.2582 - accuracy: 0.8987 - val_loss: 0.3536 - val_accuracy: 0.8515\n",
      "Epoch 5/10\n",
      "227/227 [==============================] - 2s 8ms/step - loss: 0.2410 - accuracy: 0.9070 - val_loss: 0.3632 - val_accuracy: 0.8517\n",
      "Epoch 6/10\n",
      "227/227 [==============================] - 2s 8ms/step - loss: 0.2212 - accuracy: 0.9156 - val_loss: 0.3765 - val_accuracy: 0.8491\n",
      "Epoch 7/10\n",
      "227/227 [==============================] - 2s 8ms/step - loss: 0.1955 - accuracy: 0.9278 - val_loss: 0.3907 - val_accuracy: 0.8500\n",
      "Epoch 8/10\n",
      "227/227 [==============================] - 2s 8ms/step - loss: 0.1714 - accuracy: 0.9391 - val_loss: 0.4258 - val_accuracy: 0.8457\n",
      "Epoch 9/10\n",
      "227/227 [==============================] - 2s 8ms/step - loss: 0.1465 - accuracy: 0.9502 - val_loss: 0.4447 - val_accuracy: 0.8430\n",
      "Epoch 10/10\n",
      "227/227 [==============================] - 2s 8ms/step - loss: 0.1265 - accuracy: 0.9584 - val_loss: 0.4904 - val_accuracy: 0.8406\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs = 10  \n",
    "\n",
    "history = model.fit(train_x,\n",
    "                    train_y,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "neutral-republic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 2s - loss: 0.5025 - accuracy: 0.8350\n",
      "[0.5024564266204834, 0.8349777460098267]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-stack",
   "metadata": {},
   "source": [
    "GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "fifteen-administrator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_37 (Embedding)     (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_22 (Glo (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 160,145\n",
      "Trainable params: 160,145\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab_size = 10000    # 어휘 사전의 크기입니다(10,000개의 단어)\n",
    "word_vector_dim = 16  # 워드 벡터의 차원 수 (변경 가능한 하이퍼파라미터)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.GlobalMaxPooling1D())\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "hydraulic-africa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "227/227 [==============================] - 2s 5ms/step - loss: 0.6554 - accuracy: 0.6547 - val_loss: 0.4405 - val_accuracy: 0.8211\n",
      "Epoch 2/10\n",
      "227/227 [==============================] - 1s 4ms/step - loss: 0.4013 - accuracy: 0.8364 - val_loss: 0.3602 - val_accuracy: 0.8425\n",
      "Epoch 3/10\n",
      "227/227 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8624 - val_loss: 0.3486 - val_accuracy: 0.8482\n",
      "Epoch 4/10\n",
      "227/227 [==============================] - 1s 4ms/step - loss: 0.2969 - accuracy: 0.8770 - val_loss: 0.3486 - val_accuracy: 0.8488\n",
      "Epoch 5/10\n",
      "227/227 [==============================] - 1s 4ms/step - loss: 0.2765 - accuracy: 0.8863 - val_loss: 0.3523 - val_accuracy: 0.8500\n",
      "Epoch 6/10\n",
      "227/227 [==============================] - 1s 5ms/step - loss: 0.2554 - accuracy: 0.8983 - val_loss: 0.3587 - val_accuracy: 0.8486\n",
      "Epoch 7/10\n",
      "227/227 [==============================] - 1s 4ms/step - loss: 0.2423 - accuracy: 0.9051 - val_loss: 0.3684 - val_accuracy: 0.8468\n",
      "Epoch 8/10\n",
      "227/227 [==============================] - 1s 5ms/step - loss: 0.2260 - accuracy: 0.9122 - val_loss: 0.3784 - val_accuracy: 0.8450\n",
      "Epoch 9/10\n",
      "227/227 [==============================] - 1s 4ms/step - loss: 0.2152 - accuracy: 0.9172 - val_loss: 0.3912 - val_accuracy: 0.8440\n",
      "Epoch 10/10\n",
      "227/227 [==============================] - 1s 4ms/step - loss: 0.2028 - accuracy: 0.9223 - val_loss: 0.4033 - val_accuracy: 0.8427\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs = 10  \n",
    "\n",
    "history = model.fit(train_x,\n",
    "                    train_y,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "remarkable-insight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 2s - loss: 0.4193 - accuracy: 0.8378\n",
      "[0.4192637801170349, 0.8378461003303528]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-color",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "answering-denial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_49 (Embedding)     (None, None, 200)         2000000   \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 8)                 6688      \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,006,769\n",
      "Trainable params: 2,006,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 10000  # 어휘 사전의 크기입니다(10000개의 단어)\n",
    "word_vector_dim = 200  # 단어 하나를 표현하는 임베딩 벡터의 차원수입니다. \n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,)))\n",
    "model.add(keras.layers.LSTM(8))   # 가장 널리 쓰이는 RNN인 LSTM 레이어를 사용하였습니다. 이때 LSTM state 벡터의 차원수는 8로 하였습니다. (변경 가능)\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))  # 최종 출력은 긍정/부정을 나타내는 1dim 입니다.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "affecting-zealand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "227/227 [==============================] - 8s 27ms/step - loss: 0.0591 - accuracy: 0.9807 - val_loss: 0.8041 - val_accuracy: 0.8303\n",
      "Epoch 2/10\n",
      "227/227 [==============================] - 5s 23ms/step - loss: 0.0524 - accuracy: 0.9830 - val_loss: 0.8173 - val_accuracy: 0.8301\n",
      "Epoch 3/10\n",
      "227/227 [==============================] - 5s 22ms/step - loss: 0.0510 - accuracy: 0.9835 - val_loss: 0.8502 - val_accuracy: 0.8294\n",
      "Epoch 4/10\n",
      "227/227 [==============================] - 5s 22ms/step - loss: 0.0512 - accuracy: 0.9827 - val_loss: 0.8514 - val_accuracy: 0.8286\n",
      "Epoch 5/10\n",
      "227/227 [==============================] - 5s 22ms/step - loss: 0.0518 - accuracy: 0.9827 - val_loss: 0.8346 - val_accuracy: 0.8268\n",
      "Epoch 6/10\n",
      "227/227 [==============================] - 5s 22ms/step - loss: 0.0508 - accuracy: 0.9830 - val_loss: 0.8594 - val_accuracy: 0.8281\n",
      "Epoch 7/10\n",
      "227/227 [==============================] - 5s 22ms/step - loss: 0.0481 - accuracy: 0.9840 - val_loss: 0.8258 - val_accuracy: 0.8290\n",
      "Epoch 8/10\n",
      "227/227 [==============================] - 5s 22ms/step - loss: 0.0477 - accuracy: 0.9839 - val_loss: 0.8442 - val_accuracy: 0.8276\n",
      "Epoch 9/10\n",
      "227/227 [==============================] - 5s 22ms/step - loss: 0.0466 - accuracy: 0.9839 - val_loss: 0.8571 - val_accuracy: 0.8265\n",
      "Epoch 10/10\n",
      "227/227 [==============================] - 5s 22ms/step - loss: 0.0468 - accuracy: 0.9840 - val_loss: 0.8556 - val_accuracy: 0.8260\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='RMSprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "epochs =10\n",
    "\n",
    "history = model.fit(train_x,\n",
    "                    train_y,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "norwegian-encounter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 - 4s - loss: 0.8437 - accuracy: 0.8256\n",
      "[0.8436509370803833, 0.8255792856216431]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test,  y_test, verbose=2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-employee",
   "metadata": {},
   "source": [
    "#회고 \n",
    "word2vec을 사용하는데 너무 많은 시간을 썼는데 생각보다 정확도가 많이 늘지 않아서 실망했다. 파리미터를 튜닝하면서 해나갔어야됐는데 word2vec을 사용하는데 시간을 너무 많이 써버려서 하지 못했다. 다음엔 학습 방법에 대해 더 많이 신경써야겠다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
