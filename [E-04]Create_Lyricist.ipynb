{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "coral-function",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['I hear you callin\\', \"Here I come baby\"', 'To save you, oh oh', \"Baby no more stallin'\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "glob() 함수로 lyrics 디렉토리 안에 있는 모든 파일을 읽어온다.\n",
    "\n",
    "txt 파일을 raw_corpus 리스트에 문장 단위로 이어 붙인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "numeric-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 입력된 문장\n",
    "#     1. 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "#     2. 특수문자 양쪽에 공백을 넣고\n",
    "#     3. 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "#     4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "#     5. 다시 양쪽 공백을 지웁니다\n",
    "#     6. 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-minister",
   "metadata": {},
   "source": [
    "https://dojang.io/mod/page/view.php?id=2438\n",
    "    \n",
    "re.sub으로 교체해줘야 할 문자열들을 교체해준다. 공백을 지우고, 모든 문자들을 소문자처리하고, 각 문장의 앞 뒤에 start와 end의 패딩을 저장해준다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "warming-tourism",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> i hear you callin , here i come baby <end>',\n",
       " '<start> to save you , oh oh <end>',\n",
       " '<start> baby no more stallin <end>',\n",
       " '<start> these hands have been longing to touch you baby <end>',\n",
       " '<start> and now that you ve come around , to seein it my way <end>',\n",
       " '<start> you won t regret it baby , and you surely won t forget it baby <end>',\n",
       " '<start> it s unbelieveable how your body s calling for me <end>',\n",
       " '<start> i can just hear it callin callin for me my body s callin for you <end>',\n",
       " '<start> my body s callin for you <end>',\n",
       " '<start> my body s callin for you <end>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정제 함수를 이용해 정제 데이터 구해보기\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: \n",
    "        continue\n",
    "    if len(sentence.split()) > 15: # 토큰의 개수가 15개 넘어가면 학습 데이터에서 제외\n",
    "        continue\n",
    "    corpus.append(preprocess_sentence(sentence))\n",
    "\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-emerald",
   "metadata": {},
   "source": [
    "길이가 0 혹은 15 이상의 문장들을 넘기고, \n",
    "위에서 정의했던 전처리 함수에 문장들을 넣어주고 해당 값을 corpus 변수에 저장한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "great-recipe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2   5 188 ...   0   0   0]\n",
      " [  2  10 579 ...   0   0   0]\n",
      " [  2  51  38 ...   0   0   0]\n",
      " ...\n",
      " [  2   5  92 ...   0   0   0]\n",
      " [  2   9 157 ...   0   0   0]\n",
      " [  2 163  15 ...   0   0   0]]\n",
      "<keras_preprocessing.text.Tokenizer object at 0x7fcd12594f50>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words = 12000, # 단어장 크기\n",
    "        filters = ' ',\n",
    "        oov_token = \"<unk>\" # 12000 단어에 포함되지 못한 단어\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus) # 내부 단어장\n",
    "    tensor = tokenizer.texts_to_sequences(corpus) # corpus를 Tensor로 변환\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post') # 입력 데이터의 시퀀스 길이를 일정하게 맞춰준다\n",
    "\n",
    "    print(tensor)\n",
    "    print(tokenizer)\n",
    "\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-royalty",
   "metadata": {},
   "source": [
    "https://paul-hyun.github.io/nlp-tutorial-02-02-tokenizer/ \n",
    "    \n",
    "토큰화를 이용한 자연어처리는 단어장에 단어를 저장해두고 해당 단어들을 조합하여 의미를 구성하는 것이다. \n",
    "토큰화에 있어선 글자, 띄어쓰기, 단어 등으로 하는 방법 등이 있으나 여기선 단어 단위로 나누었다.\n",
    "각 토큰은 단어의 빈도 등 각 토큰에 대한 정보가 담겨있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "naval-visiting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2   5 188   7 798   4  93   5  67  51   3   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "[  5 188   7 798   4  93   5  67  51   3   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "src_input = tensor[:, :-1] # tensor에서 마지막 토큰을 잘라내서 소스 문장 생성\n",
    "tgt_input = tensor[:, 1:] # tensor에서 <start>를 잘라내서 타겟 문장 생성\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "right-article",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (134872, 32)\n",
      "Target Train: (33718, 32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(src_input, tgt_input,test_size=0.2, random_state=42)\n",
    "print(\"Source Train:\", x_train.shape)\n",
    "print(\"Target Train:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-hindu",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test 데이터를 0.8: 0.2의 비율로 나눈다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "regulated-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size,  activation='softmax')  # added activation softmax\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-johnson",
   "metadata": {},
   "source": [
    "https://wikidocs.net/22888 \n",
    "\n",
    "언어의 데이터는 시간의 흐름을 반영하는 시계열 데이터이기 때문에, LSTM을 이용한다. \n",
    "\n",
    "싱글 RNN모델의 경우 입력단자가 많아질수록 앞의 데이터를 반영하는 것이 점점 희미해진다는 문제점이 있다. \n",
    "왜냐하면 X1을 기반으로 예측한 Y1값이 뒤로 갈수록 가중치에 계속 곱해지게 되는데, 이때의 가중치는 0~1 사이의 가중치이기 때문에 X1의 영향력이 점점 줄어들 수 밖에 없는 것이다. \n",
    "\n",
    "따라서 앞의 데이터 또한 어느 정도의 영향력을 보존할 수 있는 LSTM을 사용한다. LSTM은 단기 상태와 장기상태를 두 가자로 나누어, 정보를 처리하기 때문에 장기상태의 정보의 영향력을 보장할 수 있게 된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "patent-liechtenstein",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "396/396 [==============================] - 831s 2s/step - loss: 2.0577 - accuracy: 0.7327 - val_loss: 1.4563 - val_accuracy: 0.7708\n",
      "Epoch 2/10\n",
      "396/396 [==============================] - 820s 2s/step - loss: 1.5694 - accuracy: 0.7667 - val_loss: 1.3894 - val_accuracy: 0.7760\n",
      "Epoch 3/10\n",
      "396/396 [==============================] - 826s 2s/step - loss: 1.3412 - accuracy: 0.7782 - val_loss: 1.3267 - val_accuracy: 0.7811\n",
      "Epoch 4/10\n",
      "396/396 [==============================] - 827s 2s/step - loss: 1.2668 - accuracy: 0.7822 - val_loss: 1.2861 - val_accuracy: 0.7848\n",
      "Epoch 5/10\n",
      "396/396 [==============================] - 827s 2s/step - loss: 1.1998 - accuracy: 0.7866 - val_loss: 1.2525 - val_accuracy: 0.7888\n",
      "Epoch 6/10\n",
      "396/396 [==============================] - 823s 2s/step - loss: 1.1299 - accuracy: 0.7934 - val_loss: 1.2262 - val_accuracy: 0.7927\n",
      "Epoch 7/10\n",
      "396/396 [==============================] - 820s 2s/step - loss: 1.0652 - accuracy: 0.8006 - val_loss: 1.2056 - val_accuracy: 0.7965\n",
      "Epoch 8/10\n",
      "396/396 [==============================] - 820s 2s/step - loss: 1.0044 - accuracy: 0.8079 - val_loss: 1.1878 - val_accuracy: 0.8004\n",
      "Epoch 9/10\n",
      "396/396 [==============================] - 825s 2s/step - loss: 0.9479 - accuracy: 0.8154 - val_loss: 1.1742 - val_accuracy: 0.8037\n",
      "Epoch 10/10\n",
      "396/396 [==============================] - 825s 2s/step - loss: 0.8908 - accuracy: 0.8242 - val_loss: 1.1632 - val_accuracy: 0.8072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcc6e360b10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 256\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, validation_data=(x_test, y_test), epochs=epochs, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "tired-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델에게 시작 문장을 전달하면 모델이 시작 문장을 바탕으로 작문\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # 단어 하나씩 예측해 문장을 만듭니다\n",
    "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
    "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token:\n",
    "            break\n",
    "        if test_tensor.shape[1] >= max_len: \n",
    "            break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-voluntary",
   "metadata": {},
   "source": [
    "입력받은 문자의 다음 문자로 올 확률이 가장 높은 단어를 택하고 결과값에 이어붙인다. 이후 end 토큰이 나오거나 문장 사이즈를 넘기기전까지 계속 반복한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ethical-diana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you <end> '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "finnish-article",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i want to dig my way to hell <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i want\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "clear-northeast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i hate the way you lie <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i hate\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bearing-failure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> love me , love me <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> love\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bronze-buffer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> what you want be what you want <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> what\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "hired-response",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> in the morning , i m a go getter <end> '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> in the morning\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "afraid-zealand",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> go , get ur freak on <end> '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> go\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-departure",
   "metadata": {},
   "source": [
    "#회고 \n",
    "꽤나 그럴싸하게 문장이 나온다. 다만 아쉬운 것은 과적합의 흔적이 보인다는 것이다. love me 뒤에 또 love me가 온다거나, go get ur freak on 등은 하나의 노래에서 나온 가사들로 보이기 때문이다. 다음엔 한국어 가사를 모아서 해보면 재밌을 것 같다. 하지만 데이터 모으기가 쉽지 않겠지... 크롤링 등 데이터를 모으는 기법들에 대해서 공부를 해보아야겠다. 하면 할수록 데이터의 중요성에 대해 깨닫게 된다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
